AI Feature Selection Compares the efficiency and outcomes of several algorithms performed on a number of data sets. The difference relies on choosing the best features used to classify unknown data points. This is done using three algorithms and comparing the accuracies when choosing data points. These three algorithms are the Forwards Feature Search, Backwards Feature Search, and an algorithm that was created by myself to improve either accuracy or speed of selecting the best features. Accuracy was calculated and prepared using a leave one out partitioning method Algorithms Forward Feature Search The Forward Feature Search begins with an empty set of selected features. When the algorithm starts, it selects features one at a time to compute how accurate classification of unknown data points are using that selected feature. The best feature is the one that returns the highest accuracy, and then it is added into the set of selected features. The algorithm continues to find the best features given the set of already selected features, combined and tested with the unselected features. Backward Feature Search The Backward Feature Search begins with a set of all features. When the algorithm starts, it removes one feature from the set at a time to calculate the accuracy without a specific feature. The set that returns the highest accuracy is kept and the algorithm continues to test and remove features until the set with the best accuracy remains. Custom Algorithm (Multiple sets with random deletion) My algorithm is made to improve the accuracy of selecting the best features. This is done by taking the original data set and manipulating it so that multiple sets are created without any significant correlation to the original data set. This manipulation is deleting an X%, in this case, 5% of the data set 3 times to create 3 sets of data. Forward Feature Search is performed on these three sets with a random deletion factor added in. What is returned is 3 best feature sets, and the top features found in each is kept. Custom Algorithm AnalysisMy Algorithm is intended to improve the accuracy of choosing the best features of a data set. According to the information above, it does exactly that. The reasoning for its success is due to the random deletion done to the data before it is run through an algorithm. Additionally, multiple copies are created through this method, so each copy is resampled. A simple deletion of about 5% of data may delete spurious data instances where it looks good in one copy, but not in another. This idea of resampling encourages true and relevant data to be tested and the best features to be chosen from that data. Conclusion Performing the three algorithms among the different data sets can show us a few things. Among the three algorithms My Algorithm works the best, effectively choosing the best sets of features to provide the best accuracy. There are a few discrepancies between the small and large sets, however. Since the small data sets have fewer examples, there is more room for error. Things are generalized and classification of unknown data points is not as precise as can be. This is evident in the high percentages found when running the three algorithms on the small data sets. When running the algorithm on the large sets, we see that the classification of data points becomes more precise, and there is less room for error, resulting in lower accuracies with the first two algorithms. The abundance of training data allows the algorithm to compare against many more “nearest neighbors”. As a result, my algorithm is the most efficient of the three when it is up against more realistic data. Taking a look at the graphs,My Algorithm returns the correct features while providing a high accuracy percentage. 
Forward Search is fairly accurate. In all cases, it returns the significant features while weeding out the irrelevant ones. It becomes less accurate when more features are added into the equation. Backwards Search is, in my opinion, not a very good algorithm to use to choose the best features for classification. Although it performs decently well on the small data sets, it has a terrible rate of accuracy when compared to the other two algorithms on larger sets of data, and even fails to choose the correct features. An explanation for this performance on the small data sets is due to them not having many features.?My Algorithm performs the best overall. It is comparable to the Forward Search on small data sets, and unparalleled when tested on large data sets. Problems/Bugs There is an anomaly when returning the best feature set for Large Data set 109. I have tried re running my algorithm on Large Set 109, but each time it returns empty. My algorithm makes three copies of the data set and deletes a random 5% of data from each set. Then Forward Search is run, producing 3 best feature sets. However, the features returned from each of the copies do not match up, resulting in an empty set. A theory of mine is that I am getting unlucky each time, deleting just enough “true” data to affect the feature sets. 